{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999d478f-8da2-4fb3-96ca-5c22effbda62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T16:16:03.772982Z",
     "iopub.status.busy": "2025-11-16T16:16:03.772251Z",
     "iopub.status.idle": "2025-11-16T16:16:07.743919Z",
     "shell.execute_reply": "2025-11-16T16:16:07.737862Z",
     "shell.execute_reply.started": "2025-11-16T16:16:03.772855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env ready: willy-poc chatbot_agent customer_support_faqs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 1: Environment & clients\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "# --- Basic GCP config ---\n",
    "PROJECT_ID   = \"willy-poc\"\n",
    "BQ_LOCATION  = \"US\"\n",
    "DATASET      = \"chatbot_agent\"\n",
    "TABLE_NAME   = \"customer_support_faqs\"\n",
    "\n",
    "os.environ[\"GCLOUD_PROJECT\"] = PROJECT_ID\n",
    "\n",
    "# --- Init BigQuery client ---\n",
    "bq = bigquery.Client(project=PROJECT_ID, location=BQ_LOCATION)\n",
    "\n",
    "# --- Init Vertex AI (same project, us-central1) ---\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "# --- GenAI model for answer generation (Gemini 2.5 Pro) ---\n",
    "GEN_MODEL = GenerativeModel(\"gemini-2.5-pro\")\n",
    "GEN_CONFIG = GenerationConfig(\n",
    "    temperature=0.3,      # low-ish temperature for stable customer support tone\n",
    "    max_output_tokens=1024,\n",
    ")\n",
    "\n",
    "print(\"Env ready:\", PROJECT_ID, DATASET, TABLE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38ddbd3-b06c-4d48-ae8b-f57514a4f336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T16:16:17.701519Z",
     "iopub.status.busy": "2025-11-16T16:16:17.700424Z",
     "iopub.status.idle": "2025-11-16T16:16:19.918470Z",
     "shell.execute_reply": "2025-11-16T16:16:19.916726Z",
     "shell.execute_reply.started": "2025-11-16T16:16:17.701467Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote embedding model ready: willy-poc.chatbot_agent.text_embedding_model\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 2: Create / update remote embedding model\n",
    "# ============================================\n",
    "# This model will be used by BigQuery ML.GENERATE_EMBEDDING\n",
    "# to generate vector embeddings for FAQ content.\n",
    "# We use `gemini-embedding-001` (multilingual-friendly).\n",
    "\n",
    "# NOTE: Replace CONNECTION with your existing BQ connection name if needed.\n",
    "CONNECTION = \"us.jtcg\"  # e.g. us.jtcg / us.chatbot_agent\n",
    "\n",
    "sql_create_remote_model = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{PROJECT_ID}.{DATASET}.text_embedding_model`\n",
    "REMOTE WITH CONNECTION `{CONNECTION}`\n",
    "OPTIONS (\n",
    "  ENDPOINT = 'gemini-embedding-001'\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "bq.query(sql_create_remote_model).result()\n",
    "print(\"Remote embedding model ready:\", f\"{PROJECT_ID}.{DATASET}.text_embedding_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b03c1e91-ea3c-4a59-9988-e10f85d99ff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T16:16:42.448954Z",
     "iopub.status.busy": "2025-11-16T16:16:42.448544Z",
     "iopub.status.idle": "2025-11-16T16:16:48.817593Z",
     "shell.execute_reply": "2025-11-16T16:16:48.816467Z",
     "shell.execute_reply.started": "2025-11-16T16:16:42.448916Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensured column answer_embedding exists.\n",
      "Temporary embedding table created: willy-poc.chatbot_agent.tmp_faq_emb\n",
      "Embeddings generated & updated into willy-poc.chatbot_agent.customer_support_faqs\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 3: Add embedding column & generate embeddings\n",
    "# ============================================\n",
    "# Source table schema (already created by your previous pipeline):\n",
    "# - id (INT64)\n",
    "# - question_en / answer_en (STRING)\n",
    "# - question_zh / answer_zh (STRING)\n",
    "# - lang, source, category (STRING)\n",
    "#\n",
    "# Here we:\n",
    "# 1) Add an ARRAY<FLOAT64> column `answer_embedding` if it doesn't exist.\n",
    "# 2) Use ML.GENERATE_EMBEDDING over (question_zh + answer_zh) as content.\n",
    "# 3) Update the table with generated embeddings.\n",
    "\n",
    "# 1) Add vector column if not exists\n",
    "sql_add_column = f\"\"\"\n",
    "ALTER TABLE `{PROJECT_ID}.{DATASET}.{TABLE_NAME}`\n",
    "ADD COLUMN IF NOT EXISTS answer_embedding ARRAY<FLOAT64>;\n",
    "\"\"\"\n",
    "bq.query(sql_add_column).result()\n",
    "print(\"Ensured column answer_embedding exists.\")\n",
    "\n",
    "# 2) Generate embeddings into a temp table\n",
    "sql_tmp_emb = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET}.tmp_faq_emb` AS\n",
    "SELECT\n",
    "  id,\n",
    "  ml_generate_embedding_result AS emb\n",
    "FROM ML.GENERATE_EMBEDDING(\n",
    "  MODEL `{PROJECT_ID}.{DATASET}.text_embedding_model`,\n",
    "  (\n",
    "    SELECT\n",
    "      id,\n",
    "      CONCAT(\n",
    "        COALESCE(question_zh, ''),\n",
    "        ' ',\n",
    "        COALESCE(answer_zh, '')\n",
    "      ) AS content\n",
    "    FROM `{PROJECT_ID}.{DATASET}.{TABLE_NAME}`\n",
    "  )\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "bq.query(sql_tmp_emb).result()\n",
    "print(\"Temporary embedding table created:\", f\"{PROJECT_ID}.{DATASET}.tmp_faq_emb\")\n",
    "\n",
    "# 3) Update main table with embeddings\n",
    "sql_update_emb = f\"\"\"\n",
    "UPDATE `{PROJECT_ID}.{DATASET}.{TABLE_NAME}` f\n",
    "SET answer_embedding = t.emb\n",
    "FROM `{PROJECT_ID}.{DATASET}.tmp_faq_emb` t\n",
    "WHERE f.id = t.id;\n",
    "\"\"\"\n",
    "\n",
    "bq.query(sql_update_emb).result()\n",
    "print(\"Embeddings generated & updated into\", f\"{PROJECT_ID}.{DATASET}.{TABLE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8e99ed-cf31-4166-b607-b82642eed50a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T16:17:05.756000Z",
     "iopub.status.busy": "2025-11-16T16:17:05.754590Z",
     "iopub.status.idle": "2025-11-16T16:17:05.772595Z",
     "shell.execute_reply": "2025-11-16T16:17:05.769346Z",
     "shell.execute_reply.started": "2025-11-16T16:17:05.755940Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector search helper ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 4: Vector search helper (FAQ only, Chinese)\n",
    "# ============================================\n",
    "# We use BigQuery VECTOR_SEARCH over `answer_embedding`.\n",
    "# To keep downstream code simple, we alias:\n",
    "#   question_zh -> title\n",
    "#   answer_zh   -> answer\n",
    "#   url         -> NULL (no URL in this dataset)\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def search_knowledge(query_text: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Semantic search over FAQ table using `answer_embedding`.\n",
    "    Returns rows with:\n",
    "      - id\n",
    "      - title  (alias of question_zh)\n",
    "      - answer (alias of answer_zh)\n",
    "      - url    (currently NULL)\n",
    "      - distance (smaller = more similar)\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "    SELECT \n",
    "        base.id,\n",
    "        base.question_zh AS title,\n",
    "        base.answer_zh   AS answer,\n",
    "        NULL             AS url,\n",
    "        distance\n",
    "    FROM VECTOR_SEARCH(\n",
    "        TABLE `{PROJECT_ID}.{DATASET}.{TABLE_NAME}`,\n",
    "        'answer_embedding',\n",
    "        (\n",
    "            SELECT ml_generate_embedding_result AS query_embedding\n",
    "            FROM ML.GENERATE_EMBEDDING(\n",
    "                MODEL `{PROJECT_ID}.{DATASET}.text_embedding_model`,\n",
    "                (SELECT @query_text AS content)\n",
    "            )\n",
    "        ),\n",
    "        top_k => {top_k},\n",
    "        options => '{{\"use_brute_force\": true}}'\n",
    "    )\n",
    "    ORDER BY distance\n",
    "    \"\"\"\n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        query_parameters=[\n",
    "            bigquery.ScalarQueryParameter(\"query_text\", \"STRING\", query_text)\n",
    "        ]\n",
    "    )\n",
    "    return bq.query(sql, job_config=job_config).result()\n",
    "\n",
    "print(\"Vector search helper ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3243f1-1c42-4251-b736-531aed43d349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T16:17:32.533283Z",
     "iopub.status.busy": "2025-11-16T16:17:32.532806Z",
     "iopub.status.idle": "2025-11-16T16:17:32.546380Z",
     "shell.execute_reply": "2025-11-16T16:17:32.544874Z",
     "shell.execute_reply.started": "2025-11-16T16:17:32.533240Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM reply layer ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 5: LLM reply layer (Chinese only)\n",
    "# ============================================\n",
    "# This function wraps:\n",
    "#   - user query\n",
    "#   - retrieved FAQ blocks\n",
    "# into a single prompt for Gemini 2.5 Pro,\n",
    "# and returns the final Chinese answer.\n",
    "\n",
    "def generate_llm_reply(user_query: str, context_blocks: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Use Gemini 2.5 Pro to generate final Chinese answer for MakTek-like FAQ.\n",
    "    - user_query: original user question (Chinese)\n",
    "    - context_blocks: text blocks built from retrieved FAQ entries\n",
    "    \"\"\"\n",
    "    if not context_blocks:\n",
    "        context_text = \"（目前沒有檢索到任何 FAQ 內容。）\"\n",
    "    else:\n",
    "        context_text = \"\\n\\n\".join(\n",
    "            f\"[資料 {i+1}]\\n{block}\" for i, block in enumerate(context_blocks)\n",
    "        )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "你是一位電商平台（例如 MakTek）的官方客服 AI，請用自然、有禮貌的繁體中文回答使用者的問題。\n",
    "\n",
    "【使用者問題】\n",
    "{user_query}\n",
    "\n",
    "【系統檢索到的相關資料】\n",
    "{context_text}\n",
    "\n",
    "你的任務如下：\n",
    "\n",
    "1. 仔細閱讀以上所有「檢索到的資料」，確認其中是否已經包含對使用者問題的「明確且具體的答案」，例如：\n",
    "   - 如何註冊或登入帳號\n",
    "   - 支援哪些付款方式\n",
    "   - 如何查詢訂單進度\n",
    "   - 退貨／退款的條件與流程\n",
    "   - 是否可以取消訂單、如何操作\n",
    "   - 運送方式、運費計算、預估到貨時間 …… 等等。\n",
    "\n",
    "2. 如果你能明確判斷某一筆資料（例如 [資料 1] 或 [資料 2]）最貼近使用者問題，\n",
    "   請「優先以那一筆資料為主」整理答案，不需要平均參考所有資料，\n",
    "   以避免產生模糊或矛盾的說明。\n",
    "\n",
    "3. 只要有「任一筆」資料已經直接回應了使用者的提問，就請以那筆資料為主，\n",
    "   幫使用者整理成一段「具體而且有自信的回答」：\n",
    "   - 你可以直接引用或適度改寫該段文字；\n",
    "   - 在這種情況下，不要說「資料不足」或「無法確認」。\n",
    "\n",
    "4. 只有在「所有」檢索到的資料：\n",
    "   - 和問題主題明顯無關，或\n",
    "   - 完全沒有提供任何有用線索\n",
    "   時，你才可以回答：\n",
    "   「目前無法根據現有資料做出可靠判斷」，並建議使用者改由真人客服協助。\n",
    "\n",
    "5. 對於退貨、退款、訂單處理、付款方式、運送與運費等「政策與條款」：\n",
    "   - 你只能根據上述【系統檢索到的相關資料】內容作答；\n",
    "   - 不可以自行臆測、延伸或發明新的規則。\n",
    "\n",
    "6. 回覆時請：\n",
    "   - 先用一兩句話清楚給出「主要答案」；\n",
    "   - 再簡短補充 1–2 個重要注意事項或下一步建議（如果有）；\n",
    "   - 語氣保持專業、可信、友善，避免太冗長。\n",
    "\n",
    "最後，請只輸出要給使用者看的「最終回覆內容」：\n",
    "- 不要重複逐條列出【系統檢索到的相關資料】原文，\n",
    "- 也不要解釋你的思考過程或使用到哪些規則。\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = GEN_MODEL.generate_content(\n",
    "            prompt,\n",
    "            generation_config=GEN_CONFIG,\n",
    "        )\n",
    "        text = (response.text or \"\").strip()\n",
    "        if text:\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(\"LLM error in generate_llm_reply:\", e)\n",
    "\n",
    "    # Fallback: safe conservative message in Chinese\n",
    "    return \"抱歉，產生回答時發生問題，建議您稍後再試，或直接改由真人客服協助。\"\n",
    "\n",
    "print(\"LLM reply layer ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deba692e-89e6-4462-a65a-eef842ab5075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T16:17:50.013186Z",
     "iopub.status.busy": "2025-11-16T16:17:50.012632Z",
     "iopub.status.idle": "2025-11-16T16:17:50.032641Z",
     "shell.execute_reply": "2025-11-16T16:17:50.031112Z",
     "shell.execute_reply.started": "2025-11-16T16:17:50.013127Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ handler & main entry ready (FAQ-only RAG).\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 6: FAQ handler & main entry (answer_with_rag)\n",
    "# ============================================\n",
    "# This is a pure FAQ RAG pipeline:\n",
    "#   user query -> vector search -> (optional keyword check) -> LLM or direct answer.\n",
    "\n",
    "def handle_faq(query: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    FAQ flow:\n",
    "    1) Use BigQuery VECTOR_SEARCH to get top_k FAQs.\n",
    "    2) If the best hit is clearly on the same topic (via simple keyword check),\n",
    "       respond directly with that FAQ answer.\n",
    "    3) Otherwise, send multiple FAQ entries to LLM and let it consolidate.\n",
    "    \"\"\"\n",
    "    query = (query or \"\").strip()\n",
    "    if not query:\n",
    "        return \"\"\n",
    "\n",
    "    rows = list(search_knowledge(query, top_k=top_k))\n",
    "\n",
    "    # No FAQ at all -> tell LLM \"no context\", let it respond conservatively\n",
    "    if not rows:\n",
    "        return generate_llm_reply(query, [])\n",
    "\n",
    "    best = rows[0]\n",
    "    distance = getattr(best, \"distance\", None)\n",
    "    title    = (best.title  or \"\").strip()\n",
    "    answer   = (best.answer or \"\").strip()\n",
    "    url      = (getattr(best, \"url\", None) or \"\").strip()  # currently always empty\n",
    "\n",
    "    # Simple keyword alignment for this MakTek-like dataset (Chinese FAQ)\n",
    "    KEYWORDS = [\n",
    "        # account / login\n",
    "        \"帳號\", \"註冊\", \"登入\", \"密碼\",\n",
    "        # payment\n",
    "        \"付款\", \"付款方式\", \"信用卡\", \"金融卡\", \"PayPal\", \"支付\",\n",
    "        # order / shipping\n",
    "        \"訂單\", \"出貨\", \"運送\", \"配送\", \"運費\", \"到貨\", \"追蹤\",\n",
    "        # return / refund / cancel\n",
    "        \"退貨\", \"退款\", \"取消訂單\", \"取消\", \"退貨政策\",\n",
    "        # discount / coupon\n",
    "        \"折扣\", \"優惠碼\", \"優惠券\",\n",
    "    ]\n",
    "\n",
    "    text_for_match = title + answer\n",
    "    has_keyword_match = any((kw in query) and (kw in text_for_match) for kw in KEYWORDS)\n",
    "\n",
    "    # If semantic distance is decent + keyword aligned -> direct FAQ answer\n",
    "    if (distance is None or distance < 0.8) and has_keyword_match and answer:\n",
    "        parts = [answer]\n",
    "        # If we had URL(s), we could append them here.\n",
    "        # if url:\n",
    "        #     parts.append(f\"\\n\\n更多詳細說明，您可以參考：{url}\")\n",
    "        return \"\".join(parts).strip()\n",
    "\n",
    "    # Otherwise, build multiple context blocks and send to LLM.\n",
    "    context_blocks = []\n",
    "    for r in rows:\n",
    "        t_title    = (r.title  or \"\").strip()\n",
    "        t_answer   = (r.answer or \"\").strip()\n",
    "        t_url      = (getattr(r, \"url\", None) or \"\").strip()\n",
    "        t_distance = getattr(r, \"distance\", None)\n",
    "\n",
    "        block_lines = []\n",
    "        if t_title:\n",
    "            block_lines.append(f\"FAQ 問題：{t_title}\")\n",
    "        if t_answer:\n",
    "            block_lines.append(f\"FAQ 答案：{t_answer}\")\n",
    "        if t_url:\n",
    "            block_lines.append(f\"相關連結：{t_url}\")\n",
    "        if t_distance is not None:\n",
    "            block_lines.append(f\"語意距離（數值越小越相近）：{t_distance}\")\n",
    "\n",
    "        context_blocks.append(\"\\n\".join(block_lines))\n",
    "\n",
    "    return generate_llm_reply(query, context_blocks)\n",
    "\n",
    "\n",
    "def answer_with_rag(query_text: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Main entry:\n",
    "    - For this simplified version, we only handle FAQ-style questions.\n",
    "    - Future extensions (product recommendation / order lookup) can be added\n",
    "      as separate handlers and dispatched here based on intent.\n",
    "    \"\"\"\n",
    "    query_text = (query_text or \"\").strip()\n",
    "    if not query_text:\n",
    "        return \"\"\n",
    "    return handle_faq(query_text, top_k=top_k)\n",
    "\n",
    "print(\"FAQ handler & main entry ready (FAQ-only RAG).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea08ec1-ba4c-4ee5-886a-8b19621d9e39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T16:18:07.001368Z",
     "iopub.status.busy": "2025-11-16T16:18:07.000110Z",
     "iopub.status.idle": "2025-11-16T16:18:23.845164Z",
     "shell.execute_reply": "2025-11-16T16:18:23.843850Z",
     "shell.execute_reply.started": "2025-11-16T16:18:07.001309Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Q: 我要怎麼註冊帳號？\n",
      "\n",
      "[Vector search top-3]\n",
      "- Hit #1: id=1, distance=0.4461806710763566\n",
      "  問題：請問要怎麼註冊帳號？\n",
      "  答案：若要建立帳號，請點選我們網站右上角的「註冊」按鈕，並依照指示完成註冊程序。...\n",
      "- Hit #2: id=17, distance=0.7218236765305155\n",
      "  問題：請問可以免註冊帳號直接訂購嗎？\n",
      "  答案：是的，您不需要註冊 account，也可以直接以訪客身份下單喔。不過，若您註冊 account，將能享有更多便利，例如方...\n",
      "- Hit #3: id=161, distance=0.7383065185212451\n",
      "  問題：請問我該如何更新我的帳號資訊？\n",
      "  答案：如需更新您的會員帳號資訊，請登入您的會員帳號，點選「帳戶設定」頁面，並完成相關修改即可。...\n",
      "\n",
      "[Agent 回覆]\n",
      "若要建立帳號，請點選我們網站右上角的「註冊」按鈕，並依照指示完成註冊程序。\n",
      "\n",
      "==================================================\n",
      "Q: 你們接受哪些付款方式？\n",
      "\n",
      "[Vector search top-3]\n",
      "- Hit #1: id=2, distance=0.6041801693785498\n",
      "  問題：請問貴公司接受哪些付款方式呢？\n",
      "  答案：我們接受主要信用卡、金融卡以及 PayPal 作為線上訂單的付款方式。...\n",
      "- Hit #2: id=14, distance=0.8253338403015604\n",
      "  問題：請問我的個人資料和付款資訊有保障嗎？\n",
      "  答案：好的，我們非常重視您的個人資料與付款資訊的安全性。我們採用業界標準的加密技術，並遵循嚴格的安全協定，以確保您的資訊受到完...\n",
      "- Hit #3: id=13, distance=0.8320609226623199\n",
      "  問題：請問可以電話訂購嗎？\n",
      "  答案：不好意思，我們目前沒有提供電話訂購服務喔。建議您直接透過我們的網站下單，這樣交易過程會更順暢、更安全喔！...\n",
      "\n",
      "[Agent 回覆]\n",
      "我們接受主要信用卡、金融卡以及 PayPal 作為線上訂單的付款方式。\n",
      "\n",
      "==================================================\n",
      "Q: 我要怎麼查詢我的訂單進度？\n",
      "\n",
      "[Vector search top-3]\n",
      "- Hit #1: id=3, distance=0.4653280161344369\n",
      "  問題：請問我該如何查詢我的訂單進度呢？\n",
      "  答案：您可以登入您的會員帳號，並前往「我的訂單」頁面。您將會在那裡找到包裹的追蹤資訊。...\n",
      "- Hit #2: id=143, distance=0.7875216912589221\n",
      "  問題：請問訂單送出後，是否還能修改呢？\n",
      "  答案：如果您需要修改訂單，請盡快聯繫我們的客服團隊。若訂單尚未進入處理流程，我們將會盡力為您處理。...\n",
      "- Hit #3: id=163, distance=0.78795714881489\n",
      "  問題：請問訂單送出後，還可以修改嗎？\n",
      "  答案：如果您需要修改訂單，請盡快聯繫我們的客服團隊。如果訂單尚未處理，我們會盡力協助您處理。...\n",
      "\n",
      "[Agent 回覆]\n",
      "您可以登入您的會員帳號，並前往「我的訂單」頁面。您將會在那裡找到包裹的追蹤資訊。\n",
      "\n",
      "==================================================\n",
      "Q: 你們的退貨政策是什麼？\n",
      "\n",
      "[Vector search top-3]\n",
      "- Hit #1: id=26, distance=0.6076472672527947\n",
      "  問題：請問，如果我改變心意，可以退貨嗎？\n",
      "  答案：當然可以退貨喔！如果您改變心意，我們很樂意協助您辦理退貨。請您務必確認商品保持全新狀態且包裝完整無損，並請參考我們的退貨...\n",
      "- Hit #2: id=4, distance=0.6449757166398733\n",
      "  問題：請問貴公司的退貨政策是什麼？\n",
      "  答案：依據我們的退貨政策，您可以在購買日起30天內辦理退貨，並享有全額退款，前提是商品須保持全新狀態且包裝完整。詳細退貨說明請...\n",
      "- Hit #3: id=58, distance=0.6992184622991546\n",
      "  問題：請問商品若因不當使用造成損壞，是否可以辦理退貨呢？\n",
      "  答案：我們的退貨政策主要針對商品到貨時的瑕疵或損壞情況。若商品損壞是因不當使用所造成，則可能無法辦理退貨。如有任何疑問或需要協...\n",
      "\n",
      "[Agent 回覆]\n",
      "當然可以退貨喔！如果您改變心意，我們很樂意協助您辦理退貨。請您務必確認商品保持全新狀態且包裝完整無損，並請參考我們的退貨政策以了解詳細說明。\n",
      "\n",
      "==================================================\n",
      "Q: 已經下單了可以取消嗎？\n",
      "\n",
      "[Vector search top-3]\n",
      "- Hit #1: id=5, distance=0.5297765859250333\n",
      "  問題：我可以取消我的訂單嗎？\n",
      "  答案：若您的訂單尚未出貨，您可以申請取消。請聯繫我們的客服人員，並提供您的訂單詳情，我們將會協助您完成取消流程。...\n",
      "- Hit #2: id=19, distance=0.5451732170104064\n",
      "  問題：請問我的訂單商品可以修改或取消嗎？\n",
      "  答案：若您需要更改或取消訂單中的商品，請儘速聯繫我們的客服團隊。我們將會協助您處理相關事宜。...\n",
      "- Hit #3: id=103, distance=0.6067751087692522\n",
      "  問題：請問下單後還可以修改訂單嗎？\n",
      "  答案：如果您需要修改訂單，請盡快聯繫我們的客服團隊。如果訂單尚未處理，我們會盡力協助您。...\n",
      "\n",
      "[Agent 回覆]\n",
      "若您的訂單尚未出貨，您可以申請取消。請聯繫我們的客服人員，並提供您的訂單詳情，我們將會協助您完成取消流程。\n",
      "\n",
      "==================================================\n",
      "Q: 請問運送方式跟運費如何計算？\n",
      "\n",
      "[Vector search top-3]\n",
      "- Hit #1: id=7, distance=0.7166980060628129\n",
      "  問題：請問有提供國際運送嗎？\n",
      "  答案：是的，我們有提供國際運送服務至指定國家。實際是否可寄送以及運費，會根據您的收件地址，在結帳頁面為您計算。...\n",
      "- Hit #2: id=6, distance=0.7326554671236655\n",
      "  問題：請問運送時間大約多久呢？\n",
      "  答案：配送時間會因收件地址及您選擇的配送方式而有所不同。一般配送通常需要 3-5 個工作天，而快速配送則可能需要 1-2 個工...\n",
      "- Hit #3: id=23, distance=0.7896118050158274\n",
      "  問題：請問有提供快速配送服務嗎？\n",
      "  答案：是的，我們有提供快速運送選項，讓您能更快收到商品。在結帳流程中，您可以選擇您想要的快速運送方式。...\n",
      "\n",
      "[Agent 回覆]\n",
      "是的，我們有提供國際運送服務至指定國家。實際是否可寄送以及運費，會根據您的收件地址，在結帳頁面為您計算。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 7: Simple test cases (Chinese)\n",
    "# ============================================\n",
    "# These examples are aligned with the translated MakTek FAQ dataset.\n",
    "# They help validate:\n",
    "#   - vector search quality\n",
    "#   - direct-answer vs LLM consolidation behavior\n",
    "\n",
    "test_queries = [\n",
    "    \"我要怎麼註冊帳號？\",\n",
    "    \"你們接受哪些付款方式？\",\n",
    "    \"我要怎麼查詢我的訂單進度？\",\n",
    "    \"你們的退貨政策是什麼？\",\n",
    "    \"已經下單了可以取消嗎？\",\n",
    "    \"請問運送方式跟運費如何計算？\",\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(\"==================================================\")\n",
    "    print(\"Q:\", q)\n",
    "\n",
    "    # Show top-3 vector search hits (for debugging / article screenshots)\n",
    "    rows = list(search_knowledge(q, top_k=3))\n",
    "    print(\"\\n[Vector search top-3]\")\n",
    "    for i, r in enumerate(rows, start=1):\n",
    "        print(f\"- Hit #{i}: id={r.id}, distance={r.distance}\")\n",
    "        print(f\"  問題：{(r.title or '').strip()}\")\n",
    "        print(f\"  答案：{(r.answer or '')[:60].strip()}...\")\n",
    "    if not rows:\n",
    "        print(\"  (no hits)\")\n",
    "\n",
    "    # Show final RAG answer\n",
    "    print(\"\\n[Agent 回覆]\")\n",
    "    ans = answer_with_rag(q, top_k=5)\n",
    "    print(ans)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c072b04-6f22-43ba-861b-63961c7fb8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m135",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m135"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
